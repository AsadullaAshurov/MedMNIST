{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9KepuUKAU5S+CGlDQEQBR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsadullaAshurov/MedMNIST/blob/main/pneumonia_mnist_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install libarchive-c"
      ],
      "metadata": {
        "id": "dD_6RqUKc00n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7068f1cc-c698-49ba-c1a4-1a07a07a6adc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting libarchive-c\n",
            "  Downloading libarchive_c-5.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading libarchive_c-5.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: libarchive-c\n",
            "Successfully installed libarchive-c-5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install medmnist"
      ],
      "metadata": {
        "id": "AyjzDJofdbLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc990de-878e-495f-9322-f94dfab19273"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: medmnist in /usr/local/lib/python3.12/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from medmnist) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from medmnist) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from medmnist) (4.67.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from medmnist) (11.3.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.24.0+cpu)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->medmnist) (3.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (1.16.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (3.6.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2026.1.28)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (26.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (2025.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->medmnist) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import medmnist\n",
        "from medmnist import INFO, Evaluator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def explore_data():\n",
        "    data_flag = 'pneumoniamnist'\n",
        "    info = INFO[data_flag]\n",
        "    DataClass = getattr(medmnist, info['python_class'])\n",
        "\n",
        "    # load the data\n",
        "    train_dataset = DataClass(split='train', download=True)\n",
        "    test_dataset = DataClass(split='test', download=True)\n",
        "\n",
        "    print(f\"Data info: {info}\")\n",
        "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "    # Visualize some samples\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
        "    for i in range(5):\n",
        "        img, target = train_dataset[i]\n",
        "        axes[i].imshow(img, cmap='gray')\n",
        "        axes[i].set_title(f\"Label: {target[0]} ({info['label'][str(target[0])]})\")\n",
        "        axes[i].axis('off')\n",
        "    plt.savefig('data_samples.png')\n",
        "    print(\"Saved data_samples.png\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    explore_data()\n"
      ],
      "metadata": {
        "id": "4XskMIZkb-wm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "3edc1244-0625-4c4e-d14f-4c7a8655b7be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.17M/4.17M [00:00<00:00, 4.69MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data info: {'python_class': 'PneumoniaMNIST', 'description': 'The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.', 'url': 'https://zenodo.org/records/10519652/files/pneumoniamnist.npz?download=1', 'MD5': '28209eda62fecd6e6a2d98b1501bb15f', 'url_64': 'https://zenodo.org/records/10519652/files/pneumoniamnist_64.npz?download=1', 'MD5_64': '8f4eceb4ccffa70c672198ea285246c6', 'url_128': 'https://zenodo.org/records/10519652/files/pneumoniamnist_128.npz?download=1', 'MD5_128': '05b46931834c231683c68f40c47b2971', 'url_224': 'https://zenodo.org/records/10519652/files/pneumoniamnist_224.npz?download=1', 'MD5_224': 'd6a3c71de1b945ea11211b03746c1fe1', 'task': 'binary-class', 'label': {'0': 'normal', '1': 'pneumonia'}, 'n_channels': 1, 'n_samples': {'train': 4708, 'val': 524, 'test': 624}, 'license': 'CC BY 4.0'}\n",
            "Train dataset size: 4708\n",
            "Test dataset size: 624\n",
            "Saved data_samples.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Classification"
      ],
      "metadata": {
        "id": "qIZQN2PnlyNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup and Imports\n",
        "!pip install medmnist torch torchvision matplotlib scikit-learn captum\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.models as models\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Cell 2: Data Preparation\n",
        "data_flag = 'pneumoniamnist'\n",
        "info = INFO[data_flag]\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "])\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "train_dataset = DataClass(split='train', transform=data_transform, download=True)\n",
        "val_dataset = DataClass(split='val', transform=data_transform, download=True)\n",
        "test_dataset = DataClass(split='test', transform=data_transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "# Cell 3: Model Training\n",
        "def get_resnet18(num_classes=1):\n",
        "    model = models.resnet18(weights=None)\n",
        "    model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    model.maxpool = nn.Identity()\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "model = get_resnet18().to(device)\n",
        "\n",
        "# Handle class imbalance\n",
        "n_normal = sum(1 for _, y in train_dataset if y[0] == 0)\n",
        "n_pneumonia = sum(1 for _, y in train_dataset if y[0] == 1)\n",
        "pos_weight = torch.tensor([n_normal / n_pneumonia]).to(device)\n",
        "print(f\"Class weights - Normal: {n_normal}, Pneumonia: {n_pneumonia}, Pos weight: {pos_weight.item():.4f}\")\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 3\n",
        "train_losses = []\n",
        "val_aucs = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device).float(), target.to(device).float()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 20 == 0:\n",
        "            print(f\"Epoch {epoch}/{epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    all_targets = []\n",
        "    all_outputs = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device).float(), target.to(device).float()\n",
        "            output = model(data)\n",
        "            all_targets.append(target.cpu().numpy())\n",
        "            all_outputs.append(torch.sigmoid(output).cpu().numpy())\n",
        "\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "    all_outputs = np.concatenate(all_outputs)\n",
        "    val_auc = roc_auc_score(all_targets, all_outputs)\n",
        "    val_aucs.append(val_auc)\n",
        "    print(f\"Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Cell 4: Evaluation\n",
        "model.eval()\n",
        "test_targets = []\n",
        "test_outputs = []\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device).float(), target.to(device).float()\n",
        "        output = model(data)\n",
        "        test_targets.append(target.cpu().numpy())\n",
        "        test_outputs.append(torch.sigmoid(output).cpu().numpy())\n",
        "\n",
        "test_targets = np.concatenate(test_targets)\n",
        "test_outputs = np.concatenate(test_outputs)\n",
        "test_auc = roc_auc_score(test_targets, test_outputs)\n",
        "test_acc = np.mean((test_outputs > 0.5) == test_targets)\n",
        "print(f\"Test AUC: {test_auc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(test_targets, test_outputs > 0.5)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Pneumonia'])\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s9Q9YMJhcTLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8e599e3-245c-4677-9e03-1e64653fe8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: medmnist in /usr/local/lib/python3.12/dist-packages (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: captum in /usr/local/lib/python3.12/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from medmnist) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from medmnist) (4.67.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from medmnist) (11.3.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (26.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->medmnist) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.3)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2026.1.28)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Using device: cpu\n",
            "Loading datasets...\n",
            "Train: 4708, Val: 524, Test: 624\n",
            "Class weights - Normal: 1214, Pneumonia: 3494, Pos weight: 0.3475\n",
            "Epoch 1/3, Batch 0/74, Loss: 0.3491\n",
            "Epoch 1/3, Batch 20/74, Loss: 0.1061\n",
            "Epoch 1/3, Batch 40/74, Loss: 0.0598\n",
            "Epoch 1/3, Batch 60/74, Loss: 0.0774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.models as models\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Define the model architecture here (copied from Task 1)\n",
        "def get_resnet18(num_classes=1):\n",
        "    model = models.resnet18(weights=None)\n",
        "    # Modify for grayscale 28x28 images\n",
        "    model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    model.maxpool = nn.Identity()  # Remove maxpool since images are small\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def get_resnet_encoder():\n",
        "    \"\"\"Create encoder by removing the classifier layer from trained model\"\"\"\n",
        "    model = get_resnet18()\n",
        "\n",
        "    # Check if trained weights exist\n",
        "    if os.path.exists('resnet18_pneumonia.pth'):\n",
        "        model.load_state_dict(torch.load('resnet18_pneumonia.pth', map_location='cpu'))\n",
        "        print(\"Loaded trained weights from resnet18_pneumonia.pth\")\n",
        "    else:\n",
        "        print(\"Warning: No trained weights found. Using untrained encoder.\")\n",
        "\n",
        "    # Remove the FC layer to create encoder\n",
        "    encoder = nn.Sequential(*(list(model.children())[:-1]))\n",
        "    return encoder\n",
        "\n",
        "class ReportDataset(Dataset):\n",
        "    def __init__(self, split='train', transform=None):\n",
        "        data_flag = 'pneumoniamnist'\n",
        "        info = INFO[data_flag]\n",
        "        DataClass = getattr(medmnist, info['python_class'])\n",
        "        self.dataset = DataClass(split=split, transform=transform, download=True)\n",
        "\n",
        "        # Medical report templates\n",
        "        self.reports_normal = [\n",
        "            \"The lungs are clear. No evidence of pneumonia.\",\n",
        "            \"Normal chest X-ray. No consolidations found.\",\n",
        "            \"The heart size is normal. Lungs are well-expanded and clear.\",\n",
        "            \"No acute cardiopulmonary findings. Lungs are clear.\",\n",
        "            \"Normal examination. No evidence of infection.\"\n",
        "        ]\n",
        "        self.reports_pneumonia = [\n",
        "            \"Opacities observed in the lungs, consistent with pneumonia.\",\n",
        "            \"Bilateral consolidations found. Findings suggest pneumonia.\",\n",
        "            \"Patchy opacities in both lung fields. Findings suggest pneumonia.\",\n",
        "            \"Airspace disease noted. Consistent with pneumonia.\",\n",
        "            \"Infiltrates visible in lower lobes. Suggests pneumonia.\"\n",
        "        ]\n",
        "\n",
        "        # Build vocabulary\n",
        "        self.vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        all_text = \" \".join(self.reports_normal + self.reports_pneumonia)\n",
        "        # Clean and tokenize\n",
        "        words = []\n",
        "        for text in all_text.split():\n",
        "            clean_word = text.replace(\".\", \"\").replace(\",\", \"\").lower()\n",
        "            words.append(clean_word)\n",
        "        words = sorted(list(set(words)))\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            self.vocab[word] = i + 4\n",
        "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = [self.vocab[\"<SOS>\"]]\n",
        "        # Clean and tokenize the text\n",
        "        for word in text.replace(\".\", \"\").replace(\",\", \"\").lower().split():\n",
        "            tokens.append(self.vocab.get(word, self.vocab[\"<UNK>\"]))\n",
        "        tokens.append(self.vocab[\"<EOS>\"])\n",
        "        return torch.tensor(tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = self.dataset[idx]\n",
        "        label = target[0] if isinstance(target, np.ndarray) else target\n",
        "\n",
        "        if label == 0:\n",
        "            report = random.choice(self.reports_normal)\n",
        "        else:\n",
        "            report = random.choice(self.reports_pneumonia)\n",
        "\n",
        "        tokenized_report = self.tokenize(report)\n",
        "        return img, tokenized_report\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle variable length sequences\"\"\"\n",
        "    imgs, reports = zip(*batch)\n",
        "    imgs = torch.stack(imgs)\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    padded_reports = nn.utils.rnn.pad_sequence(reports, batch_first=True, padding_value=0)\n",
        "\n",
        "    return imgs, padded_reports\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, dropout=0.3):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features: encoded image features (batch, 512, 1, 1)\n",
        "            captions: ground truth captions (batch, seq_len)\n",
        "        \"\"\"\n",
        "        # features: (batch, 512, 1, 1) -> (batch, 1, 512)\n",
        "        features = features.view(features.size(0), 1, -1)\n",
        "\n",
        "        # Embed captions\n",
        "        embeddings = self.embed(captions[:, :-1])  # (batch, seq_len-1, embed_size)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        # Concatenate features and embeddings\n",
        "        inputs = torch.cat((features, embeddings), 1)  # (batch, seq_len, embed_size)\n",
        "\n",
        "        # Forward through GRU\n",
        "        hiddens, _ = self.gru(inputs)\n",
        "        outputs = self.linear(hiddens)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def sample(self, features, max_len=20):\n",
        "        \"\"\"\n",
        "        Generate caption greedily\n",
        "        \"\"\"\n",
        "        sampled_ids = []\n",
        "        states = None\n",
        "        inputs = features.view(features.size(0), 1, -1)  # (batch, 1, embed_size)\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            hiddens, states = self.gru(inputs, states)\n",
        "            outputs = self.linear(hiddens.squeeze(1))  # (batch, vocab_size)\n",
        "            _, predicted = outputs.max(1)  # (batch)\n",
        "\n",
        "            sampled_ids.append(predicted.item())\n",
        "\n",
        "            # Use predicted word as next input\n",
        "            inputs = self.embed(predicted).unsqueeze(1)\n",
        "\n",
        "            # Stop if <EOS> token is generated\n",
        "            if predicted.item() == 2:  # <EOS> token\n",
        "                break\n",
        "\n",
        "        return sampled_ids\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Data transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[.5], std=[.5])\n",
        "    ])\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    print(\"Loading datasets...\")\n",
        "    train_dataset = ReportDataset(split='train', transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # Initialize encoder\n",
        "    encoder = get_resnet_encoder().to(device)\n",
        "    encoder.eval()  # Keep encoder fixed (no training)\n",
        "\n",
        "    # Freeze encoder parameters\n",
        "    for param in encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Initialize decoder\n",
        "    vocab_size = len(train_dataset.vocab)\n",
        "    decoder = DecoderRNN(embed_size=512, hidden_size=512, vocab_size=vocab_size).to(device)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore <PAD> tokens\n",
        "    optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 10\n",
        "    print(\"\\nTraining decoder for report generation...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        decoder.train()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (imgs, reports) in enumerate(train_loader):\n",
        "            imgs = imgs.to(device)\n",
        "            reports = reports.to(device)\n",
        "\n",
        "            # Get image features (encoder is frozen)\n",
        "            with torch.no_grad():\n",
        "                features = encoder(imgs)  # (batch, 512, 1, 1)\n",
        "\n",
        "            # Forward through decoder\n",
        "            outputs = decoder(features, reports)  # (batch, seq_len, vocab_size)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs.reshape(-1, vocab_size), reports.reshape(-1))\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Print progress every 10 batches\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Generating reports for test samples...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Test dataset for generation\n",
        "    test_dataset = ReportDataset(split='test', transform=transform)\n",
        "\n",
        "    # Generate reports for test samples\n",
        "    for i in range(5):\n",
        "        img, _ = test_dataset[i]\n",
        "        label = test_dataset.dataset[i][1][0]\n",
        "\n",
        "        # Get image features\n",
        "        img_tensor = img.unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            features = encoder(img_tensor)\n",
        "\n",
        "            # Generate caption\n",
        "            decoder.eval()\n",
        "            sampled_ids = decoder.sample(features)\n",
        "\n",
        "        # Convert ids to words\n",
        "        words = []\n",
        "        for word_id in sampled_ids:\n",
        "            if word_id == 2:  # <EOS>\n",
        "                break\n",
        "            if word_id != 1:  # Skip <SOS>\n",
        "                words.append(train_dataset.inv_vocab.get(word_id, '<UNK>'))\n",
        "\n",
        "        report = ' '.join(words)\n",
        "        print(f\"Sample {i} (True Label: {'Pneumonia' if label == 1 else 'Normal'}):\")\n",
        "        print(f\"Generated: {report}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "TAIX-GdCcjfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "import medmnist\n",
        "from medmnist import INFO\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Define the model architecture directly in this file\n",
        "def get_resnet18(num_classes=1):\n",
        "    \"\"\"Create ResNet18 model adapted for grayscale 28x28 images\"\"\"\n",
        "    model = models.resnet18(weights=None)\n",
        "    # Modify for grayscale input (1 channel instead of 3)\n",
        "    model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    # Remove maxpool since images are small (28x28)\n",
        "    model.maxpool = nn.Identity()\n",
        "    # Modify final layer for binary classification\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def get_encoder():\n",
        "    \"\"\"Create encoder from trained ResNet18 by removing classifier\"\"\"\n",
        "    model = get_resnet18()\n",
        "\n",
        "    # Check if trained weights exist\n",
        "    weights_path = 'resnet18_pneumonia.pth'\n",
        "    if os.path.exists(weights_path):\n",
        "        model.load_state_dict(torch.load(weights_path, map_location='cpu'))\n",
        "        print(f\"Loaded trained weights from {weights_path}\")\n",
        "    else:\n",
        "        print(f\"Warning: No trained weights found at {weights_path}. Using untrained encoder.\")\n",
        "\n",
        "    # Remove the FC layer to use as feature extractor\n",
        "    # This takes all layers except the last one (classifier)\n",
        "    encoder = nn.Sequential(*(list(model.children())[:-1]))\n",
        "    return encoder\n",
        "\n",
        "def extract_features(model, loader, device):\n",
        "    \"\"\"Extract features from all images in a dataloader\"\"\"\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (imgs, targets) in enumerate(loader):\n",
        "            imgs = imgs.to(device).float()\n",
        "\n",
        "            # Extract features\n",
        "            feat = model(imgs)\n",
        "            # Flatten features: (batch, 512, 1, 1) -> (batch, 512)\n",
        "            feat = feat.view(feat.size(0), -1)\n",
        "\n",
        "            features.append(feat.cpu().numpy())\n",
        "            labels.append(targets.numpy())\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"  Processed batch {batch_idx}/{len(loader)}\")\n",
        "\n",
        "    return np.concatenate(features), np.concatenate(labels)\n",
        "\n",
        "def visualize_retrieval(query_idx, query_img, query_label, train_dataset,\n",
        "                        train_labels, top_indices, similarities, save_path=None):\n",
        "    \"\"\"Visualize query and retrieved images\"\"\"\n",
        "    fig, axes = plt.subplots(1, 6, figsize=(18, 3))\n",
        "\n",
        "    # Query image\n",
        "    axes[0].imshow(query_img[0], cmap='gray')\n",
        "    axes[0].set_title(f\"Query (Label: {'Pneumonia' if query_label==1 else 'Normal'})\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Retrieved images\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        img, _ = train_dataset[idx]\n",
        "        sim = similarities[idx]\n",
        "        label = train_labels[idx][0]\n",
        "\n",
        "        axes[i+1].imshow(img[0], cmap='gray')\n",
        "        axes[i+1].set_title(f\"Sim: {sim:.3f}\\nLabel: {'P' if label==1 else 'N'}\")\n",
        "        axes[i+1].axis('off')\n",
        "\n",
        "    plt.suptitle(f\"Top-5 Similar Images (Query #{query_idx})\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Saved figure to {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Data transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[.5], std=[.5])\n",
        "    ])\n",
        "\n",
        "    # Load dataset\n",
        "    data_flag = 'pneumoniamnist'\n",
        "    info = INFO[data_flag]\n",
        "    DataClass = getattr(medmnist, info['python_class'])\n",
        "\n",
        "    print(\"Loading datasets...\")\n",
        "    train_dataset = DataClass(split='train', transform=transform, download=True)\n",
        "    test_dataset = DataClass(split='test', transform=transform, download=True)\n",
        "\n",
        "    print(f\"Train set size: {len(train_dataset)}\")\n",
        "    print(f\"Test set size: {len(test_dataset)}\")\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "    # Initialize encoder\n",
        "    encoder = get_encoder().to(device)\n",
        "    encoder.eval()\n",
        "\n",
        "    # Extract features\n",
        "    print(\"\\nExtracting features from train set...\")\n",
        "    train_features, train_labels = extract_features(encoder, train_loader, device)\n",
        "    print(f\"Train features shape: {train_features.shape}\")\n",
        "\n",
        "    print(\"\\nExtracting features from test set...\")\n",
        "    test_features, test_labels = extract_features(encoder, test_loader, device)\n",
        "    print(f\"Test features shape: {test_features.shape}\")\n",
        "\n",
        "    # Normalize features for better cosine similarity\n",
        "    train_features = train_features / np.linalg.norm(train_features, axis=1, keepdims=True)\n",
        "    test_features = test_features / np.linalg.norm(test_features, axis=1, keepdims=True)\n",
        "\n",
        "    # Try multiple query examples\n",
        "    query_indices = [0, 10, 20, 30]  # Try different test samples\n",
        "\n",
        "    for query_idx in query_indices:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Query #{query_idx}\")\n",
        "\n",
        "        query_feat = test_features[query_idx].reshape(1, -1)\n",
        "        query_label = test_labels[query_idx][0]\n",
        "        query_img, _ = test_dataset[query_idx]\n",
        "\n",
        "        # Calculate cosine similarity with all train features\n",
        "        similarities = cosine_similarity(query_feat, train_features).flatten()\n",
        "\n",
        "        # Get top 5 most similar images\n",
        "        top_k_indices = similarities.argsort()[-5:][::-1]\n",
        "\n",
        "        print(f\"Query label: {'Pneumonia' if query_label==1 else 'Normal'}\")\n",
        "        print(\"Top matches:\")\n",
        "        for i, idx in enumerate(top_k_indices):\n",
        "            sim = similarities[idx]\n",
        "            label = train_labels[idx][0]\n",
        "            print(f\"  {i+1}. Similarity: {sim:.4f}, Label: {'Pneumonia' if label==1 else 'Normal'}\")\n",
        "\n",
        "        # Visualize\n",
        "        visualize_retrieval(\n",
        "            query_idx=query_idx,\n",
        "            query_img=query_img,\n",
        "            query_label=query_label,\n",
        "            train_dataset=train_dataset,\n",
        "            train_labels=train_labels,\n",
        "            top_indices=top_k_indices,\n",
        "            similarities=similarities,\n",
        "            save_path=f'retrieval_results_q{query_idx}.png'\n",
        "        )\n",
        "\n",
        "    # Calculate retrieval accuracy (optional)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Calculating retrieval accuracy...\")\n",
        "\n",
        "    correct_retrievals = 0\n",
        "    total_queries = min(100, len(test_features))  # Use first 100 test samples\n",
        "\n",
        "    for i in range(total_queries):\n",
        "        query_feat = test_features[i].reshape(1, -1)\n",
        "        query_label = test_labels[i][0]\n",
        "\n",
        "        similarities = cosine_similarity(query_feat, train_features).flatten()\n",
        "        top_k_indices = similarities.argsort()[-5:][::-1]\n",
        "\n",
        "        # Check if any of the top-5 retrievals have the same label\n",
        "        retrieved_labels = train_labels[top_k_indices][:, 0]\n",
        "        if query_label in retrieved_labels:\n",
        "            correct_retrievals += 1\n",
        "\n",
        "    accuracy = correct_retrievals / total_queries * 100\n",
        "    print(f\"Top-5 retrieval accuracy (same label): {accuracy:.2f}%\")\n",
        "    print(f\"(Based on {total_queries} test queries)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "z5m82T3Mclsh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}